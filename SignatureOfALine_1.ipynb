{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMK0nYdnNiOQstPV9ofOQNI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spinafex/crispy-happiness/blob/main/SignatureOfALine_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the data: here we calculate the slope and intercept which can be used to generate a hedge ratio"
      ],
      "metadata": {
        "id": "IFyB2t-5ofNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "def calculate_spread(filename):\n",
        "    data = pd.read_csv(filename)\n",
        "\n",
        "    # Check if specified columns exist\n",
        "    if 'CO1' not in data.columns or 'CL1' not in data.columns:\n",
        "        raise ValueError(f\"Columns 'CO1' and 'CL1' not found in CSV.\")\n",
        "\n",
        "    # Handle missing values by dropping rows with missing data\n",
        "    data.dropna(subset=['CO1', 'CL1'], inplace=True)  # Drop rows with NaNs in either column\n",
        "\n",
        "    # Convert pandas Series to NumPy arrays\n",
        "    d_ts1 = data['CO1'].to_numpy()\n",
        "    d_ts2 = data['CL1'].to_numpy()\n",
        "\n",
        "    # Ensure correct shape (no need for adjustments since data has no NaNs)\n",
        "    d_ts1 = np.diff(d_ts1)\n",
        "    d_ts2 = np.diff(d_ts2)\n",
        "\n",
        "    #slope, intercept = np.linalg.lstsq(d_ts1[:, np.newaxis], d_ts2, rcond=None)[0]\n",
        "    slope, intercept, _, _, _ = stats.linregress(d_ts1, d_ts2)\n",
        "\n",
        "    #return d_ts1, d_ts2\n",
        "    return slope, intercept\n",
        "\n",
        "filename = 'commodities.csv'\n",
        "slope, intercept = calculate_spread(filename)\n",
        "print(f\"{slope}, {intercept}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wm31Ap3KG5C",
        "outputId": "98c06283-4278-4a9c-929f-a95f8f5e72b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9024122927513858, 0.004363431571753323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we calculate the hedge ratio directly. The issues are the following a) the hedge ratio may not be constant and may need to be adjusted which could require a Kalman filter or some other technique, b) we are returning the spread between the differenced series (not the prices) which I think is more likely to be a OU series, c) here, we drop all rows where either asset1 or asset2 does not have data (we do not try to forward fill) which will have implications for the differenced series. Leaving them in and forward filling will have other implications so there is no perfect solution but may want to return to this at a later date to check what works best."
      ],
      "metadata": {
        "id": "LyTRopvbtW6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "def calculate_spread(filename):\n",
        "    data = pd.read_csv(filename)\n",
        "\n",
        "    # Check if specified columns exist\n",
        "    if 'CO1' not in data.columns or 'CL1' not in data.columns:\n",
        "        raise ValueError(f\"Columns 'CO1' and 'CL1' not found in CSV.\")\n",
        "\n",
        "    # Handle missing values by dropping rows with missing data\n",
        "    data.dropna(subset=['CO1', 'CL1'], inplace=True)  # Drop rows with NaNs in either column\n",
        "\n",
        "    # Convert pandas Series to NumPy arrays\n",
        "    d_ts1 = data['CO1'].to_numpy()\n",
        "    d_ts2 = data['CL1'].to_numpy()\n",
        "\n",
        "    # Ensure correct shape (no need for adjustments since data has no NaNs)\n",
        "    d_ts1 = np.diff(d_ts1)\n",
        "    d_ts2 = np.diff(d_ts2)\n",
        "\n",
        "    #slope, intercept = np.linalg.lstsq(d_ts1[:, np.newaxis], d_ts2, rcond=None)[0]\n",
        "    slope, intercept, _, _, _ = stats.linregress(d_ts1, d_ts2)\n",
        "\n",
        "    x = d_ts2 - slope * d_ts1\n",
        "    #return d_ts1, d_ts2\n",
        "    #return slope, intercept\n",
        "    return x\n",
        "\n",
        "filename = 'commodities.csv'\n",
        "x = calculate_spread(filename)\n",
        "#print(f\"{slope}, {intercept}\")\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1u4JoCoq-26",
        "outputId": "bcc95103-7eaf-482d-92f9-1eec4ac2714a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.29926535 -0.27583334  0.24949562 ...  0.34562499 -0.21563595\n",
            "  0.06196271]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Compute log signatures"
      ],
      "metadata": {
        "id": "T3EK6fHXkEQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import esig.tosig as ts\n",
        "\n",
        "def calculate_spread(filename):\n",
        "    data = pd.read_csv(filename)\n",
        "\n",
        "    # Check if specified columns exist\n",
        "    if 'CO1' not in data.columns or 'CL1' not in data.columns:\n",
        "        raise ValueError(f\"Columns 'CO1' and 'CL1' not found in CSV.\")\n",
        "\n",
        "    # Handle missing values by dropping rows with missing data\n",
        "    data.dropna(subset=['CO1', 'CL1'], inplace=True)  # Drop rows with NaNs in either column\n",
        "\n",
        "    # Convert pandas Series to NumPy arrays\n",
        "    d_ts1 = data['CO1'].to_numpy()\n",
        "    d_ts2 = data['CL1'].to_numpy()\n",
        "\n",
        "    # Ensure correct shape (no need for adjustments since data has no NaNs)\n",
        "    d_ts1 = np.diff(d_ts1)\n",
        "    d_ts2 = np.diff(d_ts2)\n",
        "\n",
        "    #slope, intercept = np.linalg.lstsq(d_ts1[:, np.newaxis], d_ts2, rcond=None)[0]\n",
        "    slope, intercept, _, _, _ = stats.linregress(d_ts1, d_ts2)\n",
        "\n",
        "    x = d_ts2 - slope * d_ts1\n",
        "\n",
        "    # Compute augmented truncated (n=3) log signature\n",
        "    signature = ts.stream2sig(x, 3)\n",
        "\n",
        "    return signature\n",
        "\n",
        "# Example usage\n",
        "filename = 'commodities.csv'\n",
        "spread_signature = calculate_spread(filename)\n",
        "print(spread_signature.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7cJyzPcxYI5",
        "outputId": "6189a896-dd0c-4aff-a247-3cddbc3988ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(585,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I do not think that this script is calculating the augmented truncated log-signature correctly"
      ],
      "metadata": {
        "id": "zgR7u22QxkQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "def calculate_spread(filename, window=10, tau=1):\n",
        "    data = pd.read_csv(filename)\n",
        "\n",
        "    # Check if specified columns exist\n",
        "    if 'CO1' not in data.columns or 'CL1' not in data.columns:\n",
        "        raise ValueError(f\"Columns 'CO1' and 'CL1' not found in CSV.\")\n",
        "\n",
        "    # Handle missing values by dropping rows with missing data\n",
        "    data.dropna(subset=['CO1', 'CL1'], inplace=True)  # Drop rows with NaNs in either column\n",
        "\n",
        "    # Convert pandas Series to NumPy arrays\n",
        "    d_ts1 = data['CO1'].to_numpy()\n",
        "    d_ts2 = data['CL1'].to_numpy()\n",
        "\n",
        "    # Ensure correct shape (no need for adjustments since data has no NaNs)\n",
        "    d_ts1 = np.diff(d_ts1)\n",
        "    d_ts2 = np.diff(d_ts2)\n",
        "\n",
        "    # Handle potential issues with differenced data length (optional)\n",
        "    if len(d_ts1) != len(d_ts2):\n",
        "        print(\"Warning: Differenced series have different lengths. Check data integrity.\")\n",
        "\n",
        "    # Perform linear regression using linregress\n",
        "    slope, intercept, _, _, _ = stats.linregress(d_ts1, d_ts2)\n",
        "\n",
        "    # Calculate spread based on fitted line\n",
        "    x = d_ts2 - slope * d_ts1\n",
        "\n",
        "    # Truncated, augmented log signatures\n",
        "    log_sig = np.log(np.abs(x) + 1e-8)  # Add small epsilon to avoid log(0)\n",
        "    truncated_log_sig = log_sig[-window:]  # Truncate to last 'window' elements\n",
        "    augmented_log_sig = np.append(truncated_log_sig, tau * np.diff(truncated_log_sig))\n",
        "\n",
        "    return augmented_log_sig\n",
        "\n",
        "\n",
        "# Example usage\n",
        "filename = 'commodities.csv'\n",
        "window = 15  # Adjust window size as needed\n",
        "tau = 0.5  # Adjust time constant as needed\n",
        "augmented_log_sig = calculate_spread(filename, window, tau)\n",
        "\n",
        "if augmented_log_sig is not None:\n",
        "    print(\"Truncated, augmented log signatures:\")\n",
        "    print(augmented_log_sig)\n",
        "else:\n",
        "    print(\"Spread calculation failed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c88Mgki5uzBF",
        "outputId": "b702f9f7-4823-4a61-f1e3-368438a704a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Truncated, augmented log signatures:\n",
            "[-2.01449176 -1.81903591 -1.48128005 -1.54788491  0.05700617 -1.35652262\n",
            " -1.61300787 -3.32125574 -2.92975542 -1.46539451 -7.23113662 -4.11226757\n",
            " -1.0624009  -1.53416365 -2.7812223   0.09772792  0.16887793 -0.03330243\n",
            "  0.80244554 -0.7067644  -0.12824262 -0.85412394  0.19575016  0.73218045\n",
            " -2.88287106  1.55943453  1.52493334 -0.23588138 -0.62352932]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Parameters\n",
        "n_samples = 1000\n",
        "n_timesteps = 100\n",
        "truncation_level_N = 3  # Truncation level for log-signature\n",
        "n_epochs = 1000\n",
        "lr = 0.01\n",
        "num_dimensions = 3\n",
        "\n",
        "# Generate time series data\n",
        "mu = 0.1\n",
        "sigma = 0.2\n",
        "dt = 1 / n_timesteps\n",
        "X0 = np.zeros((n_samples, 2))\n",
        "\n",
        "# Discretized OU process\n",
        "X = np.zeros((n_samples, n_timesteps + 1, 2))\n",
        "X[:, 0, :] = X0\n",
        "for i in range(n_samples):\n",
        "    for t in range(n_timesteps):\n",
        "        X[i, t + 1, :] = X[i, t, :] + mu * (1 - X[i, t, :]) * dt + sigma * np.sqrt(dt) * np.random.randn(2)\n",
        "\n",
        "# Payoff process (long first asset, short second asset)\n",
        "Y = X[:, :, 0] - X[:, :, 1]\n",
        "\n",
        "# Compute truncated log-signatures\n",
        "time = np.linspace(0, 1, n_timesteps + 1)\n",
        "time = np.repeat(time[None, :], n_samples, axis=0)[:, :, None]\n",
        "augmented_X = np.concatenate((time, X), axis=2)\n",
        "\n",
        "log_signatures = []\n",
        "for i in range(n_samples):\n",
        "    path = augmented_X[i]\n",
        "    log_sig = []\n",
        "    for j in range(n_timesteps + 1):\n",
        "        increment = path[j] - path[j - 1] if j > 0 else path[0]\n",
        "        num_dimensions = len(increment)\n",
        "\n",
        "        # Initialize log_sig_n with the correct size\n",
        "        if truncation_level_N == 1:\n",
        "            log_sig_n = np.zeros(num_dimensions + 1)\n",
        "        else:\n",
        "            size = int((num_dimensions ** (truncation_level_N + 1) - 1) / (num_dimensions - 1))\n",
        "            log_sig_n = np.zeros(size)\n",
        "        log_sig_n[0] = 1.0  # Initialize with 1 (empty word)\n",
        "\n",
        "        if truncation_level_N > 1:\n",
        "            for n in range(1, truncation_level_N):\n",
        "                start_idx = int((num_dimensions ** n - 1) / (num_dimensions - 1))\n",
        "                end_idx = int((num_dimensions ** (n + 1) - 1) / (num_dimensions - 1))\n",
        "                if n == 1:\n",
        "                    log_sig_n[start_idx + 1:start_idx + num_dimensions + 1] = increment\n",
        "                else:\n",
        "                    outer_product = np.multiply.outer(log_sig_n[:start_idx + 1], increment).ravel() / (n + 1)\n",
        "                    log_sig_n[start_idx + 1:end_idx + 1] = outer_product[:end_idx - start_idx]\n",
        "        else:\n",
        "            log_sig_n[1:num_dimensions + 1] = increment\n",
        "\n",
        "        log_sig.append(log_sig_n)\n",
        "    log_signatures.append(np.array(log_sig))\n",
        "log_signatures = np.array(log_signatures)\n",
        "\n",
        "#print(log_signatures)\n"
      ],
      "metadata": {
        "id": "5j18Rm9Cjjag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Optimizing loss function with neural network"
      ],
      "metadata": {
        "id": "X_zDltf5p_Xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the neural network\n",
        "class LogSignatureNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims):\n",
        "        super(LogSignatureNet, self).__init__()\n",
        "        layers = []\n",
        "        last_dim = input_dim\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(last_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            last_dim = hidden_dim\n",
        "        layers.append(nn.Linear(last_dim, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Parameters\n",
        "n_samples = 1000\n",
        "n_timesteps = 100\n",
        "truncation_level_N = 3\n",
        "n_epochs = 10 #00\n",
        "lr = 0.01\n",
        "\n",
        "# Generate dummy data for truncated log-signatures\n",
        "log_signatures = np.random.randn(n_samples, n_timesteps + 1, int(truncation_level_N * (3 ** truncation_level_N - 1) / 2))\n",
        "\n",
        "# Generate dummy payoff process\n",
        "Y = np.random.randn(n_samples, n_timesteps + 1)\n",
        "\n",
        "# Define the loss function\n",
        "def loss_fn(net):\n",
        "    loss = 0\n",
        "    for m in range(n_samples):\n",
        "        y0 = Y[m, 0]\n",
        "        integral = 0\n",
        "        for j in range(n_timesteps):\n",
        "            input_tensor = torch.tensor(log_signatures[m, j + 1], dtype=torch.float32).unsqueeze(0)\n",
        "            dnn_output = net(input_tensor).item()\n",
        "            integral += (1 - norm.cdf(integral)) * (Y[m, j + 1] - Y[m, j])\n",
        "        loss += -(y0 + integral)\n",
        "    return loss / n_samples\n",
        "\n",
        "# Optimize the loss function\n",
        "input_dim = log_signatures.shape[-1]\n",
        "hidden_dims = [5, 30, 30]  # Example architecture\n",
        "net = LogSignatureNet(input_dim, hidden_dims)\n",
        "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "for epoch in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    loss = loss_fn(net)\n",
        "    loss_tensor = torch.tensor(loss, requires_grad=True)\n",
        "    loss_tensor.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kWrkS8mp_x7",
        "outputId": "f1f5ba7b-ff82-4ad1-fe91-335e4463a38b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: -1.817154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Generate sample paths from an OU process and use these to generate a learned policy"
      ],
      "metadata": {
        "id": "RKQN6p_V5qca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Parameters\n",
        "n_samples = 750\n",
        "n_timesteps = 75\n",
        "truncation_level_N = 3\n",
        "hidden_dims = [32, 64, 32]\n",
        "max_epochs = 500\n",
        "lr = 0.001\n",
        "\n",
        "# Define the neural network architecture\n",
        "class LogSignatureNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims):\n",
        "        super(LogSignatureNet, self).__init__()\n",
        "        layers = []\n",
        "        last_dim = input_dim\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(last_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            last_dim = hidden_dim\n",
        "        layers.append(nn.Linear(last_dim, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Generate sample paths using an OU process\n",
        "def generate_sample_path(n_timesteps, mu, theta, sigma):\n",
        "    x0 = np.random.normal(mu, sigma)\n",
        "    x = np.zeros(n_timesteps + 1)\n",
        "    x[0] = x0\n",
        "    dt = 1 / n_timesteps\n",
        "    for t in range(1, n_timesteps + 1):\n",
        "        x[t] = x[t - 1] + theta * (mu - x[t - 1]) * dt + sigma * np.sqrt(dt) * np.random.normal()\n",
        "    return x\n",
        "\n",
        "# Compute the truncated log-signature\n",
        "def compute_log_signature(sample_path, truncation_level_N):\n",
        "    num_dimensions = len(sample_path[0])\n",
        "    log_sig = []\n",
        "    for j in range(len(sample_path)):\n",
        "        increment = sample_path[j] - sample_path[j - 1] if j > 0 else sample_path[0]\n",
        "        increment = torch.tensor(increment, dtype=torch.float32)  # Ensure increment is a tensor\n",
        "        size = int((num_dimensions ** (truncation_level_N + 1) - 1) / (num_dimensions - 1))\n",
        "        log_sig_n = torch.zeros(size, dtype=torch.float32)\n",
        "        log_sig_n[0] = 1.0\n",
        "        if truncation_level_N > 1:\n",
        "            for n in range(2, truncation_level_N + 1):\n",
        "                start_idx = int((num_dimensions ** n - 1) / (num_dimensions - 1))\n",
        "                end_idx = int((num_dimensions ** (n + 1) - 1) / (num_dimensions - 1))\n",
        "                if start_idx == 0:\n",
        "                    log_sig_n[1:num_dimensions + 1] = increment\n",
        "                else:\n",
        "                    outer_product = torch.outer(log_sig_n[:start_idx], increment).ravel() / n\n",
        "                    log_sig_n[start_idx:end_idx] = outer_product[:end_idx - start_idx]  # Ensure the slice size matches\n",
        "        log_sig.append(log_sig_n)\n",
        "    return torch.stack(log_sig)\n",
        "\n",
        "# Define the loss function\n",
        "def loss_fn(net, log_signatures, sample_paths):\n",
        "    loss = 0\n",
        "    for m in range(n_samples):\n",
        "        y0 = sample_paths[m, 0]\n",
        "        integral = 0\n",
        "        for j in range(n_timesteps):\n",
        "            input_tensor = log_signatures[m, j + 1].unsqueeze(0)\n",
        "            dnn_output = net(input_tensor).item()\n",
        "            integral += (1 - norm.cdf(integral)) * (sample_paths[m, j + 1] - sample_paths[m, j])\n",
        "        loss += -(y0 + integral)\n",
        "    return loss / n_samples\n",
        "\n",
        "# Generate training data\n",
        "mu = 0.1\n",
        "theta = 0.5\n",
        "sigma = 0.2\n",
        "sample_paths = torch.tensor([generate_sample_path(n_timesteps, mu, theta, sigma) for _ in range(n_samples)], dtype=torch.float32)\n",
        "log_signatures = torch.stack([compute_log_signature(sample_path, truncation_level_N) for sample_path in sample_paths])\n",
        "\n",
        "# Initialize the neural network and optimizer\n",
        "input_dim = log_signatures.shape[-1]\n",
        "net = LogSignatureNet(input_dim, hidden_dims)\n",
        "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "# Train the neural network\n",
        "for epoch in range(max_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    loss = loss_fn(net, log_signatures, sample_paths)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Return the learned policy\n",
        "def learned_policy(net, test_log_signature):\n",
        "    with torch.no_grad():\n",
        "        policy_output = net(test_log_signature.unsqueeze(0)).item()\n",
        "    return policy_output\n",
        "\n",
        "# Example usage: Evaluate the learned policy on test data\n",
        "test_sample_path = torch.tensor(generate_sample_path(n_timesteps, mu, theta, sigma), dtype=torch.float32, device='cuda')\n",
        "test_log_signature = compute_log_signature(test_sample_path, truncation_level_N)\n",
        "\n",
        "policy_output = learned_policy(net, test_log_signature)\n",
        "print(f\"Policy output for test data: {policy_output}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "ubAw0IZjsXKU",
        "outputId": "5a1426d7-31ce-49c3-adde-a1c1d45e19fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "len() of a 0-d tensor",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-9bd3e4b3294c>\u001b[0m in \u001b[0;36m<cell line: 80>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0msample_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgenerate_sample_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mlog_signatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_log_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation_level_N\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# Initialize the neural network and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-9bd3e4b3294c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0msample_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgenerate_sample_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mlog_signatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_log_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation_level_N\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# Initialize the neural network and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-9bd3e4b3294c>\u001b[0m in \u001b[0;36mcompute_log_signature\u001b[0;34m(sample_path, truncation_level_N)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Compute the truncated log-signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_log_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation_level_N\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mnum_dimensions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mlog_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"len() of a 0-d tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mTypeError\u001b[0m: len() of a 0-d tensor"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Parameters\n",
        "n_samples = 750\n",
        "n_timesteps = 75\n",
        "truncation_level_N = 3\n",
        "hidden_dims = [32, 64, 32]\n",
        "max_epochs = 500\n",
        "lr = 0.001\n",
        "\n",
        "# Define the neural network architecture\n",
        "class LogSignatureNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims):\n",
        "        super(LogSignatureNet, self).__init__()\n",
        "        layers = []\n",
        "        last_dim = input_dim\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(last_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            last_dim = hidden_dim\n",
        "        layers.append(nn.Linear(last_dim, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Generate sample paths using an OU process\n",
        "def generate_sample_path(n_timesteps, mu, theta, sigma):\n",
        "    x0 = np.random.normal(mu, sigma)\n",
        "    x = np.zeros(n_timesteps + 1)\n",
        "    x[0] = x0\n",
        "    dt = 1 / n_timesteps\n",
        "    for t in range(1, n_timesteps + 1):\n",
        "        x[t] = x[t - 1] + theta * (mu - x[t - 1]) * dt + sigma * np.sqrt(dt) * np.random.normal()\n",
        "    return x\n",
        "\n",
        "# Compute the truncated log-signature\n",
        "def compute_log_signature(sample_path, truncation_level_N):\n",
        "    sample_path = torch.tensor(sample_path, dtype=torch.float32).clone().detach().requires_grad_(True)  # Ensure sample_path is a tensor\n",
        "    num_dimensions = 1  # Since sample_path is 1-dimensional\n",
        "    log_sig = []\n",
        "    for j in range(sample_path.shape[0]):\n",
        "        increment = sample_path[j] - sample_path[j - 1] if j > 0 else sample_path[0]\n",
        "        increment = increment.clone().detach().requires_grad_(True).reshape(-1)  # Ensure increment is a 1D tensor\n",
        "        if num_dimensions == 1:\n",
        "            size = truncation_level_N + 1\n",
        "        else:\n",
        "            size = int((num_dimensions ** (truncation_level_N + 1) - 1) / (num_dimensions - 1))\n",
        "        log_sig_n = torch.zeros(size, dtype=torch.float32)\n",
        "        log_sig_n[0] = 1.0  # Set initial value before requires_grad\n",
        "        log_sig_n = log_sig_n.clone().detach().requires_grad_(True)\n",
        "        if truncation_level_N > 1:\n",
        "            for n in range(2, truncation_level_N + 1):\n",
        "                if num_dimensions == 1:\n",
        "                    # Use slicing to create new tensors instead of in-place operations\n",
        "                    log_sig_n = torch.cat((log_sig_n[:1], increment[:n], log_sig_n[n+1:]))\n",
        "                else:\n",
        "                    start_idx = int((num_dimensions ** n - 1) / (num_dimensions - 1))\n",
        "                    end_idx = int((num_dimensions ** (n + 1) - 1) / (num_dimensions - 1))\n",
        "                    outer_product = torch.outer(log_sig_n[:start_idx], increment).ravel() / n\n",
        "                    log_sig_n = torch.cat((log_sig_n[:start_idx], outer_product[:end_idx - start_idx], log_sig_n[end_idx:]))  # Ensure the slice size matches\n",
        "        log_sig.append(log_sig_n)\n",
        "    return torch.stack(log_sig)\n",
        "\n",
        "# Define the loss function\n",
        "def loss_fn(net, log_signatures, sample_paths):\n",
        "    loss = 0\n",
        "    for m in range(n_samples):\n",
        "        y0 = sample_paths[m, 0]\n",
        "        integral = 0\n",
        "        for j in range(n_timesteps):\n",
        "            input_tensor = log_signatures[m, j + 1].unsqueeze(0)\n",
        "            dnn_output = net(input_tensor)\n",
        "            integral += (1 - norm.cdf(integral)) * (sample_paths[m, j + 1] - sample_paths[m, j])\n",
        "        loss += -(y0 + integral)\n",
        "    return loss / n_samples\n",
        "\n",
        "# Generate training data\n",
        "mu = 0.1\n",
        "theta = 0.5\n",
        "sigma = 0.2\n",
        "sample_paths = np.array([generate_sample_path(n_timesteps, mu, theta, sigma) for _ in range(n_samples)])\n",
        "sample_paths = torch.tensor(sample_paths, dtype=torch.float32)\n",
        "log_signatures = torch.stack([compute_log_signature(sample_path, truncation_level_N) for sample_path in sample_paths])\n",
        "\n",
        "# Initialize the neural network and optimizer\n",
        "input_dim = log_signatures.shape[-1]\n",
        "net = LogSignatureNet(input_dim, hidden_dims)\n",
        "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "# Train the neural network\n",
        "for epoch in range(max_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    loss = loss_fn(net, log_signatures, sample_paths)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Return the learned policy\n",
        "def learned_policy(net, test_log_signature):\n",
        "    with torch.no_grad():\n",
        "        policy_output = net(test_log_signature.unsqueeze(0)).item()\n",
        "    return policy_output\n",
        "\n",
        "# Example usage: Evaluate the learned policy on test data\n",
        "test_sample_path = generate_sample_path(n_timesteps, mu, theta, sigma)\n",
        "test_log_signature = compute_log_signature(test_sample_path, truncation_level_N)\n",
        "\n",
        "policy_output = learned_policy(net, test_log_signature)\n",
        "print(f\"Policy output for test data: {policy_output}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "5WSN1Nvl_uLZ",
        "outputId": "39afb4de-7632-47cf-e2ba-e303167d27c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-53-0c846199381d>:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sample_path = torch.tensor(sample_path, dtype=torch.float32).clone().detach().requires_grad_(True)  # Ensure sample_path is a tensor\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-0c846199381d>\u001b[0m in \u001b[0;36m<cell line: 96>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_signatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from scipy.stats import norm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import esig.tosig as ts\n",
        "\n",
        "# Define the neural network\n",
        "class LogSignatureNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims):\n",
        "        super(LogSignatureNet, self).__init__()\n",
        "        layers = []\n",
        "        last_dim = input_dim\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(last_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            last_dim = hidden_dim\n",
        "        layers.append(nn.Linear(last_dim, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "def calculate_spread(filename):\n",
        "    data = pd.read_csv(filename)\n",
        "\n",
        "    # Check if specified columns exist\n",
        "    if 'CO1' not in data.columns or 'CL1' not in data.columns:\n",
        "        raise ValueError(f\"Columns 'CO1' and 'CL1' not found in CSV.\")\n",
        "\n",
        "    # Handle missing values by dropping rows with missing data\n",
        "    data.dropna(subset=['CO1', 'CL1'], inplace=True)  # Drop rows with NaNs in either column\n",
        "\n",
        "    # Convert pandas Series to NumPy arrays\n",
        "    d_ts1 = data['CO1'].to_numpy()\n",
        "    d_ts2 = data['CL1'].to_numpy()\n",
        "\n",
        "    # Ensure correct shape (no need for adjustments since data has no NaNs)\n",
        "    d_ts1 = np.diff(d_ts1)\n",
        "    d_ts2 = np.diff(d_ts2)\n",
        "\n",
        "    # Compute slope and intercept of the linear regression\n",
        "    slope, intercept, _, _, _ = stats.linregress(d_ts1, d_ts2)\n",
        "\n",
        "    # Calculate the spread\n",
        "    x = d_ts2 - slope * d_ts1\n",
        "\n",
        "    return x\n",
        "\n",
        "# Parameters\n",
        "n_samples = 100 #1000\n",
        "n_timesteps = 50 #100\n",
        "truncation_level_N = 3\n",
        "n_epochs = 10 #0\n",
        "lr = 0.01\n",
        "block_size = 10  # Block size for block bootstrapping\n",
        "\n",
        "# Load data and calculate the spread\n",
        "filename = 'commodities.csv'\n",
        "x = calculate_spread(filename)\n",
        "\n",
        "# Generate sample log-signatures using block bootstrapping\n",
        "log_signatures = []\n",
        "for _ in range(n_samples):\n",
        "    bootstrap_indices = np.random.choice(len(x), size=(n_timesteps,), replace=True)\n",
        "    bootstrap_data = x[bootstrap_indices]\n",
        "    signature = ts.stream2sig(bootstrap_data, truncation_level_N)\n",
        "    log_signatures.append(signature)\n",
        "\n",
        "log_signatures = np.array(log_signatures)\n",
        "\n",
        "# Generate dummy payoff process\n",
        "Y = np.random.randn(n_samples, n_timesteps + 1)\n",
        "\n",
        "# Define the loss function\n",
        "def loss_fn(net):\n",
        "    loss = 0\n",
        "    for m in range(n_samples):\n",
        "        y0 = Y[m, 0]\n",
        "        integral = 0\n",
        "        for j in range(n_timesteps):\n",
        "            input_tensor = torch.tensor(log_signatures[m, j], dtype=torch.float32).unsqueeze(0)\n",
        "            dnn_output = net(input_tensor).item()\n",
        "            integral += (1 - norm.cdf(integral)) * (Y[m, j + 1] - Y[m, j])\n",
        "        loss += -(y0 + integral)\n",
        "    return loss / n_samples\n",
        "\n",
        "# Optimize the loss function\n",
        "input_dim = log_signatures.shape[-1]\n",
        "hidden_dims = [5, 30, 30]  # Example architecture\n",
        "net = LogSignatureNet(input_dim, hidden_dims)\n",
        "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "for epoch in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    loss = loss_fn(net)\n",
        "    loss_tensor = torch.tensor(loss, requires_grad=True)\n",
        "    loss_tensor.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.6f}\")\n"
      ],
      "metadata": {
        "id": "91o5M-hO6jDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from scipy.stats import norm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import esig.tosig as ts\n",
        "\n",
        "# Define the neural network\n",
        "class LogSignatureNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims):\n",
        "        super(LogSignatureNet, self).__init__()\n",
        "        layers = []\n",
        "        last_dim = input_dim\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(last_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            last_dim = hidden_dim\n",
        "        layers.append(nn.Linear(last_dim, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "def calculate_spread(filename):\n",
        "    try:\n",
        "        data = pd.read_csv(filename)\n",
        "\n",
        "        # Check if specified columns exist\n",
        "        if 'CO1' not in data.columns or 'CL1' not in data.columns:\n",
        "            raise ValueError(f\"Columns 'CO1' and 'CL1' not found in CSV.\")\n",
        "\n",
        "        # Handle missing values by dropping rows with missing data\n",
        "        data.dropna(subset=['CO1', 'CL1'], inplace=True)  # Drop rows with NaNs in either column\n",
        "\n",
        "        # Convert pandas Series to NumPy arrays\n",
        "        d_ts1 = data['CO1'].to_numpy()\n",
        "        d_ts2 = data['CL1'].to_numpy()\n",
        "\n",
        "        # Ensure correct shape (no need for adjustments since data has no NaNs)\n",
        "        d_ts1 = np.diff(d_ts1)\n",
        "        d_ts2 = np.diff(d_ts2)\n",
        "\n",
        "        # Compute slope and intercept of the linear regression\n",
        "        slope, intercept, _, _, _ = stats.linregress(d_ts1, d_ts2)\n",
        "\n",
        "        # Calculate the spread\n",
        "        x = d_ts2 - slope * d_ts1\n",
        "\n",
        "        return x\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading data: {e}\")\n",
        "        return None\n",
        "\n",
        "# Parameters\n",
        "n_samples = 500  # Reduced number of samples\n",
        "n_timesteps = 50  # Reduced number of timesteps\n",
        "truncation_level_N = 3\n",
        "n_epochs = 50  # Reduced number of epochs\n",
        "lr = 0.01\n",
        "block_size = 5  # Reduced block size\n",
        "\n",
        "# Load data and calculate the spread\n",
        "filename = 'commodities.csv'\n",
        "x = calculate_spread(filename)\n",
        "\n",
        "if x is not None:\n",
        "    # Generate sample log-signatures using block bootstrapping\n",
        "    log_signatures = []\n",
        "    for _ in range(n_samples):\n",
        "        bootstrap_indices = np.random.choice(len(x), size=(n_timesteps,), replace=True)\n",
        "        bootstrap_data = x[bootstrap_indices]\n",
        "        signature = ts.stream2sig(bootstrap_data, truncation_level_N)\n",
        "        log_signatures.append(signature)\n",
        "\n",
        "    log_signatures = np.array(log_signatures)\n",
        "\n",
        "    # Generate dummy payoff process\n",
        "    Y = np.random.randn(n_samples, n_timesteps + 1)\n",
        "\n",
        "    # Define the loss function\n",
        "    def loss_fn(net):\n",
        "        loss = 0\n",
        "        for m in range(n_samples):\n",
        "            y0 = Y[m, 0]\n",
        "            integral = 0\n",
        "            for j in range(n_timesteps):\n",
        "                input_tensor = torch.tensor(log_signatures[m, j], dtype=torch.float32).unsqueeze(0)\n",
        "                dnn_output = net(input_tensor).item()\n",
        "                integral += (1 - norm.cdf(integral)) * (Y[m, j + 1] - Y[m, j])\n",
        "            loss += -(y0 + integral)\n",
        "        return loss / n_samples\n",
        "\n",
        "    # Optimize the loss function\n",
        "    input_dim = log_signatures.shape[-1]\n",
        "    hidden_dims = [5, 30, 30]  # Example architecture\n",
        "    net = LogSignatureNet(input_dim, hidden_dims)\n",
        "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "    for epoch in range(n_epochs):\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(net)\n",
        "        loss_tensor = torch.tensor(loss, requires_grad=True)\n",
        "        loss_tensor.backward()\n",
        "        optimizer.step()\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item():.6f}\")\n",
        "else:\n",
        "    print(\"Error: Unable to process data. Check file or data format.\")\n"
      ],
      "metadata": {
        "id": "kMFsm9jL8XDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "e0oFoZZXGyR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install esig"
      ],
      "metadata": {
        "id": "qto8vFK1G3rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy"
      ],
      "metadata": {
        "id": "usfkJdGuZEUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute slope, intercept and spread series"
      ],
      "metadata": {
        "id": "zvSNUaAPp5Qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import linregress\n",
        "\n",
        "# Load the data from the CSV file\n",
        "data = pd.read_csv('commodities.csv', parse_dates=['Date'])\n",
        "\n",
        "# Forward fill missing values\n",
        "data = data.ffill()\n",
        "\n",
        "# Extract the CO1 and CL1 series\n",
        "co1 = data['CO1']\n",
        "cl1 = data['CL1']\n",
        "\n",
        "# Calculate the differences between the two series\n",
        "diffs = co1 - cl1\n",
        "\n",
        "# Remove NaN values from the differences\n",
        "diffs = diffs.dropna()\n",
        "\n",
        "# Compute the linear regression\n",
        "slope, intercept, _, _, _ = linregress(diffs.index, diffs.values)\n",
        "\n",
        "# Calculate the spread series\n",
        "spread = diffs - (slope * diffs.index + intercept)\n",
        "\n",
        "print(f\"Slope: {slope}\")\n",
        "print(f\"Intercept: {intercept}\")\n",
        "print(f\"Spread Series:\\n{spread}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0YrmZD8Nlx9",
        "outputId": "7bdc81ac-8072-4d70-a5b9-c78c43e7d573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope: -0.00041589295632048613\n",
            "Intercept: 4.631841205045514\n",
            "Spread Series:\n",
            "0       4.218159\n",
            "1       3.908575\n",
            "2       4.258991\n",
            "3       3.949406\n",
            "4       4.309822\n",
            "          ...   \n",
            "1304    0.530483\n",
            "1305    0.440899\n",
            "1306    0.151315\n",
            "1307    0.271731\n",
            "1308    0.252147\n",
            "Length: 1309, dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-108-4c67706da66f>:6: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
            "  data = pd.read_csv('commodities.csv', parse_dates=['Date'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def calculate_spread_series(data):\n",
        "    \"\"\"\n",
        "    Calculates the spread series between CL1 and CO1 based on their linear relationship.\n",
        "\n",
        "    Args:\n",
        "        data (pandas.DataFrame): DataFrame containing the CO1 and CL1 columns.\n",
        "\n",
        "    Returns:\n",
        "        pandas.Series: The spread series defined by CL1 - [slope * CO1].\n",
        "    \"\"\"\n",
        "    # Forward fill missing values\n",
        "    data = data.ffill()\n",
        "\n",
        "    # Extract the CO1 and CL1 series\n",
        "    co1 = data['CO1']\n",
        "    cl1 = data['CL1']\n",
        "\n",
        "    # Calculate the differences between the two series\n",
        "    diffs_co1 = co1.diff()\n",
        "    diffs_cl1 = cl1.diff()\n",
        "\n",
        "    # Remove NaN values from the differences\n",
        "    diffs_co1 = diffs_co1.dropna()\n",
        "    diffs_cl1 = diffs_cl1.dropna()\n",
        "\n",
        "    # Compute the slope of the linear relationship\n",
        "    slope = diffs_cl1.cov(diffs_co1) / diffs_co1.var()\n",
        "\n",
        "    # Calculate the spread series\n",
        "    spread_series = cl1 - slope * co1\n",
        "\n",
        "    return spread_series\n",
        "\n",
        "# Load the data from the CSV file\n",
        "data = pd.read_csv('commodities.csv')\n",
        "\n",
        "# Calculate the spread series\n",
        "spread = calculate_spread_series(data)\n",
        "\n",
        "print(\"Spread Series:\\n\", spread)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_XnMwR5ffkW",
        "outputId": "75bddc4f-c650-4d0e-be1f-a9ffab35e7dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spread Series:\n",
            " 0      -1.178478\n",
            "1      -0.880174\n",
            "2      -1.149365\n",
            "3      -0.905288\n",
            "4      -1.417337\n",
            "          ...   \n",
            "1304    4.298742\n",
            "1305    4.272845\n",
            "1306    4.623452\n",
            "1307    4.399251\n",
            "1308    4.464972\n",
            "Length: 1309, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XEhzFh1mjkO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Parameters\n",
        "n_samples = 750\n",
        "n_timesteps = 75\n",
        "n_features = 3  # dimensions of the truncated log signatures\n",
        "\n",
        "# Define Ornstein-Uhlenbeck (OU) process\n",
        "def ou_process(n_timesteps):\n",
        "    theta = 0.1\n",
        "    mu = 0.5\n",
        "    sigma = 0.3\n",
        "    dt = 0.1\n",
        "    x = np.zeros(n_timesteps)\n",
        "    for t in range(1, n_timesteps):\n",
        "        x[t] = x[t-1] + theta * (mu - x[t-1]) * dt + sigma * np.sqrt(dt) * np.random.normal()\n",
        "    return x\n",
        "\n",
        "# Generate sample paths\n",
        "sample_paths = np.array([ou_process(n_timesteps) for _ in range(n_samples)])\n",
        "\n",
        "# Compute augmented truncated log signatures\n",
        "log_signatures = np.random.randn(n_samples, n_timesteps+1, n_features)  # Just random for demonstration\n",
        "\n",
        "# Define the neural network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc = nn.Linear(n_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Define the loss function\n",
        "def loss_fn(net, log_signatures, sample_paths):\n",
        "    loss = 0\n",
        "    for m in range(n_samples):\n",
        "        y0 = sample_paths[m, 0]\n",
        "        integral = 0\n",
        "        for j in range(n_timesteps - 1):  # Adjusted range to avoid out-of-bounds index\n",
        "            input_tensor = torch.tensor(log_signatures[m, j + 1]).unsqueeze(0).float()\n",
        "            dnn_output = net(input_tensor)\n",
        "            integral += (1 - norm.cdf(integral)) * (sample_paths[m, j + 1] - sample_paths[m, j])\n",
        "        loss += -(y0 + integral)\n",
        "    return torch.tensor(loss / n_samples, requires_grad=True)\n",
        "\n",
        "# Initialize the neural network\n",
        "net = Net()\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    loss = loss_fn(net, log_signatures, sample_paths)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Once training is done, you can deploy this network against test or live sequential data\n",
        "# The learned policy is encoded in the weights of the neural network\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HCr0HlZJGw7u",
        "outputId": "7d80ffb9-29e1-444a-a4bd-56b92fbf2ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: -0.14608828588923656\n",
            "Epoch 2/100, Loss: -0.14608828588923656\n",
            "Epoch 3/100, Loss: -0.14608828588923656\n",
            "Epoch 4/100, Loss: -0.14608828588923656\n",
            "Epoch 5/100, Loss: -0.14608828588923656\n",
            "Epoch 6/100, Loss: -0.14608828588923656\n",
            "Epoch 7/100, Loss: -0.14608828588923656\n",
            "Epoch 8/100, Loss: -0.14608828588923656\n",
            "Epoch 9/100, Loss: -0.14608828588923656\n",
            "Epoch 10/100, Loss: -0.14608828588923656\n",
            "Epoch 11/100, Loss: -0.14608828588923656\n",
            "Epoch 12/100, Loss: -0.14608828588923656\n",
            "Epoch 13/100, Loss: -0.14608828588923656\n",
            "Epoch 14/100, Loss: -0.14608828588923656\n",
            "Epoch 15/100, Loss: -0.14608828588923656\n",
            "Epoch 16/100, Loss: -0.14608828588923656\n",
            "Epoch 17/100, Loss: -0.14608828588923656\n",
            "Epoch 18/100, Loss: -0.14608828588923656\n",
            "Epoch 19/100, Loss: -0.14608828588923656\n",
            "Epoch 20/100, Loss: -0.14608828588923656\n",
            "Epoch 21/100, Loss: -0.14608828588923656\n",
            "Epoch 22/100, Loss: -0.14608828588923656\n",
            "Epoch 23/100, Loss: -0.14608828588923656\n",
            "Epoch 24/100, Loss: -0.14608828588923656\n",
            "Epoch 25/100, Loss: -0.14608828588923656\n",
            "Epoch 26/100, Loss: -0.14608828588923656\n",
            "Epoch 27/100, Loss: -0.14608828588923656\n",
            "Epoch 28/100, Loss: -0.14608828588923656\n",
            "Epoch 29/100, Loss: -0.14608828588923656\n",
            "Epoch 30/100, Loss: -0.14608828588923656\n",
            "Epoch 31/100, Loss: -0.14608828588923656\n",
            "Epoch 32/100, Loss: -0.14608828588923656\n",
            "Epoch 33/100, Loss: -0.14608828588923656\n",
            "Epoch 34/100, Loss: -0.14608828588923656\n",
            "Epoch 35/100, Loss: -0.14608828588923656\n",
            "Epoch 36/100, Loss: -0.14608828588923656\n",
            "Epoch 37/100, Loss: -0.14608828588923656\n",
            "Epoch 38/100, Loss: -0.14608828588923656\n",
            "Epoch 39/100, Loss: -0.14608828588923656\n",
            "Epoch 40/100, Loss: -0.14608828588923656\n",
            "Epoch 41/100, Loss: -0.14608828588923656\n",
            "Epoch 42/100, Loss: -0.14608828588923656\n",
            "Epoch 43/100, Loss: -0.14608828588923656\n",
            "Epoch 44/100, Loss: -0.14608828588923656\n",
            "Epoch 45/100, Loss: -0.14608828588923656\n",
            "Epoch 46/100, Loss: -0.14608828588923656\n",
            "Epoch 47/100, Loss: -0.14608828588923656\n",
            "Epoch 48/100, Loss: -0.14608828588923656\n",
            "Epoch 49/100, Loss: -0.14608828588923656\n",
            "Epoch 50/100, Loss: -0.14608828588923656\n",
            "Epoch 51/100, Loss: -0.14608828588923656\n",
            "Epoch 52/100, Loss: -0.14608828588923656\n",
            "Epoch 53/100, Loss: -0.14608828588923656\n",
            "Epoch 54/100, Loss: -0.14608828588923656\n",
            "Epoch 55/100, Loss: -0.14608828588923656\n",
            "Epoch 56/100, Loss: -0.14608828588923656\n",
            "Epoch 57/100, Loss: -0.14608828588923656\n",
            "Epoch 58/100, Loss: -0.14608828588923656\n",
            "Epoch 59/100, Loss: -0.14608828588923656\n",
            "Epoch 60/100, Loss: -0.14608828588923656\n",
            "Epoch 61/100, Loss: -0.14608828588923656\n",
            "Epoch 62/100, Loss: -0.14608828588923656\n",
            "Epoch 63/100, Loss: -0.14608828588923656\n",
            "Epoch 64/100, Loss: -0.14608828588923656\n",
            "Epoch 65/100, Loss: -0.14608828588923656\n",
            "Epoch 66/100, Loss: -0.14608828588923656\n",
            "Epoch 67/100, Loss: -0.14608828588923656\n",
            "Epoch 68/100, Loss: -0.14608828588923656\n",
            "Epoch 69/100, Loss: -0.14608828588923656\n",
            "Epoch 70/100, Loss: -0.14608828588923656\n",
            "Epoch 71/100, Loss: -0.14608828588923656\n",
            "Epoch 72/100, Loss: -0.14608828588923656\n",
            "Epoch 73/100, Loss: -0.14608828588923656\n",
            "Epoch 74/100, Loss: -0.14608828588923656\n",
            "Epoch 75/100, Loss: -0.14608828588923656\n",
            "Epoch 76/100, Loss: -0.14608828588923656\n",
            "Epoch 77/100, Loss: -0.14608828588923656\n",
            "Epoch 78/100, Loss: -0.14608828588923656\n",
            "Epoch 79/100, Loss: -0.14608828588923656\n",
            "Epoch 80/100, Loss: -0.14608828588923656\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-70338ff5020b>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_signatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-70338ff5020b>\u001b[0m in \u001b[0;36mloss_fn\u001b[0;34m(net, log_signatures, sample_paths)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_signatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdnn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mintegral\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintegral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msample_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msample_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mintegral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py\u001b[0m in \u001b[0;36mcdf\u001b[0;34m(self, x, *args, **kwds)\u001b[0m\n\u001b[1;32m   2066\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtyp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2067\u001b[0m         \u001b[0mcond0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_argcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2068\u001b[0;31m         \u001b[0mcond1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_support_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2069\u001b[0m         \u001b[0mcond2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mcond0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcond0\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mcond1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py\u001b[0m in \u001b[0;36m_open_support_mask\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_open_support_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/_ufunc_config.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *exc_info)\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moldcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseterrcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mseterr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moldstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_Unspecified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Track the progress/time toward convergence/completion"
      ],
      "metadata": {
        "id": "CnELkYF7BCv7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WzaJruZvBRFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Track the profit of the trade"
      ],
      "metadata": {
        "id": "NJN5936m9TxB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PiEM4STU9qMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Plot the enter/exit points against the spread"
      ],
      "metadata": {
        "id": "CNMd5hVA9TrD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XEOcAJXV9qnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "a5lOIE-f9TmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gEzBQWWe9Ti8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D_EXspFJsXG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ClT11IxMsXDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8rFFMFRasXAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wf9Ee0L4sW9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QuXPsN-8sW6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gkDwO6DYsW3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bPEDxXo0sW0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K2n1AFuwsWw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R5FG31qrsWrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Optimize the loss function using NNs"
      ],
      "metadata": {
        "id": "o9CHKpnumVwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Parameters\n",
        "n_samples = 1000\n",
        "n_timesteps = 100\n",
        "truncation_level_N = 3  # Truncation level for log-signature\n",
        "n_epochs = 1000\n",
        "lr = 0.01\n",
        "num_dimensions = 3\n",
        "\n",
        "# Generate time series data\n",
        "mu = 0.1\n",
        "sigma = 0.2\n",
        "dt = 1 / n_timesteps\n",
        "X0 = np.zeros((n_samples, 2))\n",
        "\n",
        "# Discretized OU process\n",
        "X = np.zeros((n_samples, n_timesteps + 1, 2))\n",
        "X[:, 0, :] = X0\n",
        "for i in range(n_samples):\n",
        "    for t in range(n_timesteps):\n",
        "        X[i, t + 1, :] = X[i, t, :] + mu * (1 - X[i, t, :]) * dt + sigma * np.sqrt(dt) * np.random.randn(2)\n",
        "\n",
        "# Payoff process (long first asset, short second asset)\n",
        "Y = X[:, :, 0] - X[:, :, 1]\n",
        "\n",
        "# Compute truncated log-signatures\n",
        "time = np.linspace(0, 1, n_timesteps + 1)\n",
        "time = np.repeat(time[None, :], n_samples, axis=0)[:, :, None]\n",
        "augmented_X = np.concatenate((time, X), axis=2)\n",
        "\n",
        "log_signatures = []\n",
        "for i in range(n_samples):\n",
        "    path = augmented_X[i]\n",
        "    log_sig = []\n",
        "    for j in range(n_timesteps + 1):\n",
        "        increment = path[j] - path[j - 1] if j > 0 else path[0]\n",
        "        num_dimensions = len(increment)\n",
        "\n",
        "        # Initialize log_sig_n with the correct size\n",
        "        if truncation_level_N == 1:\n",
        "            log_sig_n = np.zeros(num_dimensions + 1)\n",
        "        else:\n",
        "            size = int((num_dimensions ** (truncation_level_N + 1) - 1) / (num_dimensions - 1))\n",
        "            log_sig_n = np.zeros(size)\n",
        "        log_sig_n[0] = 1.0  # Initialize with 1 (empty word)\n",
        "\n",
        "        if truncation_level_N > 1:\n",
        "            for n in range(1, truncation_level_N):\n",
        "                start_idx = int((num_dimensions ** n - 1) / (num_dimensions - 1))\n",
        "                end_idx = int((num_dimensions ** (n + 1) - 1) / (num_dimensions - 1))\n",
        "                if n == 1:\n",
        "                    log_sig_n[start_idx + 1:start_idx + num_dimensions + 1] = increment\n",
        "                else:\n",
        "                    outer_product = np.multiply.outer(log_sig_n[:start_idx + 1], increment).ravel() / (n + 1)\n",
        "                    log_sig_n[start_idx + 1:end_idx + 1] = outer_product[:end_idx - start_idx]\n",
        "        else:\n",
        "            log_sig_n[1:num_dimensions + 1] = increment\n",
        "\n",
        "        log_sig.append(log_sig_n)\n",
        "    log_signatures.append(np.array(log_sig))\n",
        "log_signatures = np.array(log_signatures)\n",
        "\n",
        "# Convert log_signatures to PyTorch tensors\n",
        "log_signatures = torch.tensor(log_signatures, dtype=torch.float32)\n",
        "Y = torch.tensor(Y, dtype=torch.float32)\n",
        "\n",
        "# Define the neural network\n",
        "class LogSignatureNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims):\n",
        "        super(LogSignatureNet, self).__init__()\n",
        "        layers = []\n",
        "        last_dim = input_dim\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(last_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            last_dim = hidden_dim\n",
        "        layers.append(nn.Linear(last_dim, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Define the loss function\n",
        "def loss_fn(net):\n",
        "    loss = 0\n",
        "    for m in range(n_samples):\n",
        "        y0 = Y[m, 0]\n",
        "        integral = 0\n",
        "        for j in range(n_timesteps):\n",
        "            # Ensure the input tensor requires gradients\n",
        "            input_tensor = log_signatures[m, j + 1].unsqueeze(0).requires_grad_(True)\n",
        "            dnn_output = net(input_tensor).item()\n",
        "            integral += (1 - norm.cdf(integral)) * (Y[m, j + 1] - Y[m, j])\n",
        "        loss += -(y0 + integral)\n",
        "    return loss / n_samples\n",
        "\n",
        "# Optimize the loss function\n",
        "input_dim = log_signatures.shape[-1]\n",
        "hidden_dims = [5, 30, 30]  # Example architecture\n",
        "net = LogSignatureNet(input_dim, hidden_dims)\n",
        "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "for epoch in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    loss = loss_fn(net)\n",
        "    # Ensure the loss tensor requires gradients\n",
        "    loss = torch.tensor(loss, requires_grad=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.6f}\")\n",
        "\n",
        "print(f\"Optimal deep signature stopping policy parameters: {list(net.parameters())}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "Ei37d9x4OPdK",
        "outputId": "35f06b92-c412-4e86-c240-43985a061f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-a5aff9d5efd4>:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = torch.tensor(loss, requires_grad=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.003443\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-a5aff9d5efd4>\u001b[0m in \u001b[0;36m<cell line: 107>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Ensure the loss tensor requires gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-a5aff9d5efd4>\u001b[0m in \u001b[0;36mloss_fn\u001b[0;34m(net)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_signatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mdnn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mintegral\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintegral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mintegral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7zj-6x0wOPgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lD1HzEDzcZuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hz2sxKgicZrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "egJXG0C6cZo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aSFvmPPPcZl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uoc_mzLccZim"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}