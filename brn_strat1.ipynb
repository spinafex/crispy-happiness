{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDM9GFx2urHCzL7oxZrcPd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spinafex/crispy-happiness/blob/main/brn_strat1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g4nElG8QHTAB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "from statsmodels.tsa.stattools import coint\n",
        "from scipy.stats import zscore\n",
        "import statsmodels.api as sm\n",
        "from scipy.optimize import minimize\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from a CSV file\n",
        "def load_data(file_path):\n",
        "    data = pd.read_csv(file_path, index_col=0, parse_dates=True, dayfirst=True)\n",
        "    data = data.interpolate(method='linear', axis=0, limit_direction='both', limit=5)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Split data into training and testing sets\n",
        "def split_data(data, split_ratio=0.5):\n",
        "    split_point = int(len(data) * split_ratio)\n",
        "    train_data = data[:split_point]\n",
        "    test_data = data[split_point:]\n",
        "    #print(train_data) #debugger\n",
        "    return train_data, test_data\n",
        "\n",
        "# Calculate daily returns for the given data\n",
        "def calculate_daily_returns(data):\n",
        "    # Convert the data to a NumPy array\n",
        "    data_array = data.values\n",
        "\n",
        "    # Check if the first row contains labels or numeric data\n",
        "    if data_array.dtype.kind == 'O':\n",
        "        # First row contains labels\n",
        "        labels = data_array[0].tolist()\n",
        "        data_array = data_array[1:]\n",
        "    else:\n",
        "        # First row contains numeric data\n",
        "        labels = data.columns.tolist()\n",
        "        #print(labels) #debug\n",
        "\n",
        "    # Calculate the daily returns\n",
        "    pct_change = [(x - y) / y for x, y in zip(data_array[1:], data_array[:-1])]\n",
        "    pct_change = np.array(pct_change)\n",
        "\n",
        "    return labels, pct_change\n",
        "\n",
        "def find_cointegrated_pairs(labels, pct_change):\n",
        "    # Convert pct_change to a NumPy array if necessary\n",
        "    if not isinstance(pct_change, np.ndarray):\n",
        "        pct_change = np.array(pct_change)\n",
        "\n",
        "    # Drop rows with NaN or inf values\n",
        "    pct_change = pct_change[~np.isnan(pct_change).any(axis=1)]\n",
        "    pct_change = pct_change[~np.isinf(pct_change).any(axis=1)]\n",
        "\n",
        "    cointegrated_pairs = []\n",
        "\n",
        "    # Iterate over each pair of assets\n",
        "    for i in range(pct_change.shape[1]):\n",
        "        for j in range(i + 1, pct_change.shape[1]):\n",
        "            asset1 = pct_change[:, i]\n",
        "            asset2 = pct_change[:, j]\n",
        "\n",
        "            # Test for cointegration\n",
        "            score, pvalue, _ = coint(asset1, asset2)\n",
        "\n",
        "            # Check if the pair is cointegrated\n",
        "            if pvalue < 0.05:\n",
        "                cointegrated_pairs.append((labels[i], labels[j], pvalue))\n",
        "\n",
        "    return cointegrated_pairs\n",
        "\n",
        "file_path = 'crypto.csv'\n",
        "data = load_data(file_path)\n",
        "train_data, _ = split_data(data, split_ratio=0.5)\n",
        "labels, pct_change = calculate_daily_returns(train_data)\n",
        "cointegrated_pairs = find_cointegrated_pairs(labels, pct_change)\n",
        "print(cointegrated_pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNveS1JiHUWr",
        "outputId": "a34735f5-b6b6-4a68-84fd-eb86d0710044"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('BTC', 'ETH', 6.7886492716240616e-09), ('BTC', 'DOGE', 1.469532910382539e-09), ('BTC', 'ADA ', 1.506304231032239e-25), ('BTC', 'SHIB', 6.820111003321325e-09), ('BTC', 'AVAX', 3.3500908304090666e-14), ('BTC', 'LINK', 7.39353263321604e-29), ('BTC', 'TRX', 6.794200675970872e-14), ('BTC', 'UNI', 5.350516600090906e-09), ('BTC', 'MATIC', 1.0204408377940847e-06), ('ETH', 'DOGE', 3.373856812438888e-11), ('ETH', 'ADA ', 1.4090152874910037e-25), ('ETH', 'SHIB', 7.10292868411349e-25), ('ETH', 'AVAX', 1.518570828200927e-11), ('ETH', 'LINK', 2.1457965400527525e-25), ('ETH', 'TRX', 2.2122839496054053e-29), ('ETH', 'UNI', 8.694383397478782e-25), ('ETH', 'MATIC', 1.1891945421798322e-06), ('DOGE', 'ADA ', 1.2284616609566376e-18), ('DOGE', 'SHIB', 1.216780274347241e-16), ('DOGE', 'AVAX', 1.7687683535111327e-17), ('DOGE', 'LINK', 6.076406978866814e-18), ('DOGE', 'TRX', 2.373433289950151e-17), ('DOGE', 'UNI', 2.5654201073693612e-17), ('DOGE', 'MATIC', 1.204275070566345e-17), ('ADA ', 'SHIB', 2.0809425633027375e-29), ('ADA ', 'AVAX', 0.0), ('ADA ', 'LINK', 2.5008250000332858e-29), ('ADA ', 'TRX', 0.0), ('ADA ', 'UNI', 1.941825953119933e-29), ('ADA ', 'MATIC', 3.90038460198076e-19), ('SHIB', 'AVAX', 1.4472668089923823e-29), ('SHIB', 'LINK', 1.4497001542492e-29), ('SHIB', 'TRX', 1.482714795434507e-29), ('SHIB', 'UNI', 1.450984954718702e-29), ('SHIB', 'MATIC', 1.4689218096550586e-29), ('AVAX', 'LINK', 3.15911920815746e-17), ('AVAX', 'TRX', 6.437377078222063e-17), ('AVAX', 'UNI', 5.632970829560006e-17), ('AVAX', 'MATIC', 1.522313816081181e-16), ('LINK', 'TRX', 1.2382833514543402e-26), ('LINK', 'UNI', 6.426591874167038e-26), ('LINK', 'MATIC', 2.084408095335203e-16), ('TRX', 'UNI', 0.0), ('TRX', 'MATIC', 0.0001001481247806198), ('UNI', 'MATIC', 0.0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from statsmodels.tsa.stattools import coint\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "import pandas as pd\n",
        "\n",
        "# Load data from a CSV file\n",
        "def load_data(file_path):\n",
        "    data = pd.read_csv(file_path, index_col=0, parse_dates=True, dayfirst=True)\n",
        "    data = data.interpolate(method='linear', axis=0, limit_direction='both', limit=5)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Split data into training and testing sets\n",
        "def split_data(data, split_ratio=0.5):\n",
        "    split_point = int(len(data) * split_ratio)\n",
        "    train_data = data[:split_point]\n",
        "    test_data = data[split_point:]\n",
        "    #print(train_data) #debugger\n",
        "    return train_data, test_data\n",
        "\n",
        "# Calculate daily returns for the given data\n",
        "def calculate_daily_returns(data):\n",
        "    # Convert the data to a NumPy array\n",
        "    data_array = data.values\n",
        "\n",
        "    # Check if the first row contains labels or numeric data\n",
        "    if data_array.dtype.kind == 'O':\n",
        "        # First row contains labels\n",
        "        labels = data_array[0].tolist()\n",
        "        data_array = data_array[1:]\n",
        "    else:\n",
        "        # First row contains numeric data\n",
        "        labels = data.columns.tolist()\n",
        "        #print(labels) #debug\n",
        "\n",
        "    # Calculate the daily returns\n",
        "    pct_change = [(x - y) / y for x, y in zip(data_array[1:], data_array[:-1])]\n",
        "    pct_change = np.array(pct_change)\n",
        "\n",
        "    return labels, pct_change\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from statsmodels.tsa.stattools import adfuller, coint\n",
        "\n",
        "def find_cointegrated_pairs(labels, pct_change):\n",
        "    # Convert pct_change to NumPy array if necessary\n",
        "    if not isinstance(pct_change, np.ndarray):\n",
        "        pct_change = np.array(pct_change)\n",
        "\n",
        "    # Drop rows with NaN or inf values\n",
        "    pct_change = pct_change[~np.isnan(pct_change).any(axis=1)]\n",
        "    pct_change = pct_change[~np.isinf(pct_change).any(axis=1)]\n",
        "\n",
        "    n = pct_change.shape[1]\n",
        "    corr_matrix = np.zeros((n, n))\n",
        "    p_value_matrix = np.ones((n, n))\n",
        "    np.fill_diagonal(p_value_matrix, np.nan)\n",
        "\n",
        "    stationarity_results = []\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            asset1 = pct_change[:, i]\n",
        "            asset2 = pct_change[:, j]\n",
        "\n",
        "            # Calculate correlation coefficient\n",
        "            corr_coef = np.corrcoef(asset1, asset2)[0, 1]\n",
        "\n",
        "            # Test for stationarity\n",
        "            adf_stat, p_val, _, _, _, _ = adfuller(asset1 - asset2)\n",
        "            is_stationary = p_val < 0.05\n",
        "\n",
        "            # Test for cointegration\n",
        "            _, coint_pvalue, _ = coint(asset1, asset2)\n",
        "\n",
        "            corr_matrix[i, j] = corr_coef\n",
        "            corr_matrix[j, i] = corr_coef\n",
        "            p_value_matrix[i, j] = coint_pvalue\n",
        "            p_value_matrix[j, i] = coint_pvalue\n",
        "\n",
        "            # Store the stationarity test result\n",
        "            pair_label = f\"{labels[i]} - {labels[j]}\"\n",
        "            result = {\n",
        "                'Pair': pair_label,\n",
        "                'ADF Statistic': adf_stat,\n",
        "                'p-value': p_val,\n",
        "                'Is Stationary': is_stationary\n",
        "            }\n",
        "            stationarity_results.append(result)\n",
        "\n",
        "    corr_df = pd.DataFrame(corr_matrix, columns=labels, index=labels)\n",
        "    p_value_df = pd.DataFrame(p_value_matrix, columns=labels, index=labels)\n",
        "    stationarity_results_df = pd.DataFrame(stationarity_results)\n",
        "\n",
        "    return corr_df, p_value_df, stationarity_results_df\n",
        "\n",
        "\n",
        "    corr_df = pd.DataFrame(corr_matrix, columns=labels, index=labels)\n",
        "    p_value_df = pd.DataFrame(p_value_matrix, columns=labels, index=labels)\n",
        "    results = [list(result) for result in results]\n",
        "    results = pd.DataFrame(results, columns=labels, index=labels)\n",
        "\n",
        "    return corr_df, p_value_df, results\n",
        "\n",
        "\n",
        "\n",
        "file_path = 'crypto.csv'\n",
        "data = load_data(file_path)\n",
        "train_data, _ = split_data(data, split_ratio=0.5)\n",
        "labels, pct_change = calculate_daily_returns(train_data)\n",
        "corr_df, p_value_df, results = find_cointegrated_pairs(labels, pct_change)\n",
        "#print(corr_df)\n",
        "#corr_df.to_csv('corr_matrix', index=True, header=True)\n",
        "#p_value_df.to_csv('pvalue_matrix', index=True, header=True)\n",
        "results.to_csv('adf', index=True, header=True)"
      ],
      "metadata": {
        "id": "lfK3sjiseewF"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from statsmodels.tsa.stattools import coint\n",
        "import pandas as pd\n",
        "\n",
        "# Load data from a CSV file\n",
        "def load_data(file_path):\n",
        "    data = pd.read_csv(file_path, index_col=0, parse_dates=True, dayfirst=True)\n",
        "    data = data.interpolate(method='linear', axis=0, limit_direction='both', limit=5)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Split data into training and testing sets\n",
        "def split_data(data, split_ratio=0.5):\n",
        "    split_point = int(len(data) * split_ratio)\n",
        "    train_data = data[:split_point]\n",
        "    test_data = data[split_point:]\n",
        "    #print(train_data) #debugger\n",
        "    return train_data, test_data\n",
        "\n",
        "# Calculate daily returns for the given data\n",
        "def calculate_daily_returns(data):\n",
        "    # Convert the data to a NumPy array\n",
        "    data_array = data.values\n",
        "\n",
        "    # Check if the first row contains labels or numeric data\n",
        "    if data_array.dtype.kind == 'O':\n",
        "        # First row contains labels\n",
        "        labels = data_array[0].tolist()\n",
        "        data_array = data_array[1:]\n",
        "    else:\n",
        "        # First row contains numeric data\n",
        "        labels = data.columns.tolist()\n",
        "        #print(labels) #debug\n",
        "\n",
        "    # Calculate the daily returns\n",
        "    pct_change = [(x - y) / y for x, y in zip(data_array[1:], data_array[:-1])]\n",
        "    pct_change = np.array(pct_change)\n",
        "\n",
        "    return labels, pct_change\n",
        "\n",
        "# Find cointegrated pairs\n",
        "def find_cointegrated_pairs(train_data):\n",
        "    returns = calculate_daily_returns(train_data)\n",
        "    n = returns.shape[1]\n",
        "    keys = returns.columns\n",
        "    pairs = []\n",
        "    pvalues = np.ones((n, n))\n",
        "    for i, j in itertools.combinations(range(n), 2):\n",
        "        S1 = returns[keys[i]]\n",
        "        S2 = returns[keys[j]]\n",
        "        result = coint(S1, S2)\n",
        "        pvalue = result[1]\n",
        "        pvalues[i, j] = pvalue\n",
        "        if pvalue < 0.05:\n",
        "            pairs.append((keys[i], keys[j], pvalue))\n",
        "    return pairs, pvalues"
      ],
      "metadata": {
        "id": "EzNOZhgbHUT1"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colorama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_YXAUZshWw0",
        "outputId": "030d1d2b-f34d-4e2a-bb1e-7c6dcdddb865"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from statsmodels.tsa.stattools import coint\n",
        "import pandas as pd\n",
        "\n",
        "# Load data from a CSV file\n",
        "def load_data(file_path):\n",
        "    data = pd.read_csv(file_path, index_col=0, parse_dates=True, dayfirst=True)\n",
        "    data = data.interpolate(method='linear', axis=0, limit_direction='both', limit=5)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Split data into training and testing sets\n",
        "def split_data(data, split_ratio=0.5):\n",
        "    split_point = int(len(data) * split_ratio)\n",
        "    train_data = data[:split_point]\n",
        "    test_data = data[split_point:]\n",
        "    #print(train_data) #debugger\n",
        "    return train_data, test_data\n",
        "\n",
        "# Calculate daily returns for the given data\n",
        "def calculate_daily_returns(data):\n",
        "    # Convert the data to a NumPy array\n",
        "    data_array = data.values\n",
        "\n",
        "    # Check if the first row contains labels or numeric data\n",
        "    if data_array.dtype.kind == 'O':\n",
        "        # First row contains labels\n",
        "        labels = data_array[0].tolist()\n",
        "        data_array = data_array[1:]\n",
        "    else:\n",
        "        # First row contains numeric data\n",
        "        labels = data.columns.tolist()\n",
        "        #print(labels) #debug\n",
        "\n",
        "    # Calculate the daily returns\n",
        "    pct_change = [(x - y) / y for x, y in zip(data_array[1:], data_array[:-1])]\n",
        "    pct_change = np.array(pct_change)\n",
        "\n",
        "    return labels, pct_change\n",
        "\n",
        "def find_cointegrated_pairs(labels, pct_change):\n",
        "    print(\"Entering find_cointegrated_pairs function\")\n",
        "\n",
        "    # Convert pct_change to NumPy array if necessary\n",
        "    if not isinstance(pct_change, np.ndarray):\n",
        "        pct_change = np.array(pct_change)\n",
        "        print(\"Converted pct_change to NumPy array\")\n",
        "    else:\n",
        "        print(\"pct_change is already a NumPy array\")\n",
        "\n",
        "    # Drop rows with NaN or inf values\n",
        "    original_shape = pct_change.shape\n",
        "    pct_change = pct_change[~np.isnan(pct_change).any(axis=1)]\n",
        "    pct_change = pct_change[~np.isinf(pct_change).any(axis=1)]\n",
        "    print(f\"Dropped {original_shape[0] - pct_change.shape[0]} rows with NaN or inf values\")\n",
        "\n",
        "    cointegrated_pairs = []\n",
        "\n",
        "    # Iterate over each pair of assets\n",
        "    for i in range(pct_change.shape[1]):\n",
        "        for j in range(i + 1, pct_change.shape[1]):\n",
        "            asset1 = pct_change[:, i]\n",
        "            asset2 = pct_change[:, j]\n",
        "\n",
        "            # Test for cointegration\n",
        "            score, pvalue, _ = coint(asset1, asset2)\n",
        "\n",
        "            # Check if the pair is cointegrated\n",
        "            if pvalue < 0.05:\n",
        "                cointegrated_pairs.append((labels[i], labels[j], pvalue))\n",
        "\n",
        "    if not cointegrated_pairs:\n",
        "        print(\"No cointegrated pairs found.\")\n",
        "    else:\n",
        "        print(\"Cointegrated pairs:\")\n",
        "        for pair in cointegrated_pairs:\n",
        "            print(f\"{pair[0]} and {pair[1]} (p-value: {pair[2]:.16f})\")\n",
        "\n",
        "    return cointegrated_pairs\n",
        "\n",
        "\n",
        "file_path = 'crypto.csv'\n",
        "data = load_data(file_path)\n",
        "train_data, _ = split_data(data, split_ratio=0.5)\n",
        "labels, pct_change = calculate_daily_returns(train_data)\n",
        "cointegrated_pairs = find_cointegrated_pairs(labels, pct_change)\n",
        "#print(p_value_df)"
      ],
      "metadata": {
        "id": "PZLNzZgeHxBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OU Process"
      ],
      "metadata": {
        "id": "Wxl_zDbh1MTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "def find_optimal_hedge_ratio(asset1, asset2, max_iter=1000):\n",
        "    \"\"\"\n",
        "    Find the hedge ratio that minimizes the mean-reverting speed of the spread between two assets.\n",
        "\n",
        "    Args:\n",
        "        asset1 (numpy.ndarray): The first asset's time series data.\n",
        "        asset2 (numpy.ndarray): The second asset's time series data.\n",
        "        max_iter (int): The maximum number of iterations for the optimization algorithm.\n",
        "\n",
        "    Returns:\n",
        "        float: The optimal hedge ratio.\n",
        "        float: The mean-reverting speed of the spread with the optimal hedge ratio.\n",
        "    \"\"\"\n",
        "    def objective_function(hedge_ratio):\n",
        "        spread = asset1 - hedge_ratio * asset2\n",
        "        adf_stat, _, _, _, _, _ = adfuller(spread)\n",
        "        return -adf_stat  # Minimize the negative ADF statistic (maximize the ADF statistic)\n",
        "\n",
        "    initial_guess = 1.0  # Initial guess for the hedge ratio\n",
        "    result = minimize(objective_function, initial_guess, method='Nelder-Mead', options={'maxiter': max_iter})\n",
        "    optimal_hedge_ratio = result.x[0]\n",
        "    spread = asset1 - optimal_hedge_ratio * asset2\n",
        "    adf_stat, _, _, _, _, _ = adfuller(spread)\n",
        "    mean_reverting_speed = -adf_stat\n",
        "\n",
        "    return optimal_hedge_ratio, mean_reverting_speed\n",
        "\n",
        "def find_optimal_hedge_ratios(data, labels):\n",
        "    \"\"\"\n",
        "    Find the optimal hedge ratios and mean-reverting speeds for all pairs of assets.\n",
        "\n",
        "    Args:\n",
        "        data (pandas.DataFrame): A DataFrame containing the asset data.\n",
        "        labels (list): A list of asset labels.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: A DataFrame containing the optimal hedge ratios and mean-reverting speeds for each pair of assets.\n",
        "    \"\"\"\n",
        "    n = len(labels)\n",
        "    results = []\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            asset1 = data.iloc[:, i].values\n",
        "            asset2 = data.iloc[:, j].values\n",
        "\n",
        "            # Check for inf or NaN values\n",
        "            if np.isnan(asset1).any() or np.isnan(asset2).any() or np.isinf(asset1).any() or np.isinf(asset2).any():\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                optimal_hedge_ratio, mean_reverting_speed = find_optimal_hedge_ratio(asset1, asset2)\n",
        "                pair_label = f\"{labels[i]} - {labels[j]}\"\n",
        "                result = {\n",
        "                    'Pair': pair_label,\n",
        "                    'Optimal Hedge Ratio': optimal_hedge_ratio,\n",
        "                    'Mean-Reverting Speed': mean_reverting_speed\n",
        "                }\n",
        "                results.append(result)\n",
        "            except (ValueError, np.linalg.LinAlgError):\n",
        "                # Ignore pairs with errors or NaNs\n",
        "                continue\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "file_path = 'crypto.csv'\n",
        "data = load_data(file_path)\n",
        "train_data, _ = split_data(data, split_ratio=0.5)\n",
        "labels, _ = calculate_daily_returns(train_data)\n",
        "#cointegrated_pairs = find_cointegrated_pairs(labels, pct_change)\n",
        "#print(p_value_df)\n",
        "hedge_ratios = find_optimal_hedge_ratios(train_data, labels)\n",
        "print(hedge_ratios)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsEUXAZaiSBz",
        "outputId": "67ab3451-9e29-451b-ae12-12ab3a8789cc"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Pair  Optimal Hedge Ratio  Mean-Reverting Speed\n",
            "0      BTC - ETH        -4.193887e+01              0.148451\n",
            "1     BTC - DOGE         8.539004e+01              0.780816\n",
            "2     BTC - ADA         -2.059356e+03              0.733710\n",
            "3     BTC - LINK         2.229229e+02              0.724089\n",
            "4      BTC - TRX        -3.763027e+04              0.757796\n",
            "5      BTC - UNI         8.575059e+01              0.725373\n",
            "6    BTC - MATIC        -5.403157e+04             -0.625676\n",
            "7     ETH - DOGE         3.878019e+03             -1.476407\n",
            "8     ETH - ADA         -1.458988e+02             -0.241431\n",
            "9     ETH - LINK         6.142500e+01             -2.564072\n",
            "10     ETH - TRX         8.652258e+03             -0.435223\n",
            "11     ETH - UNI         4.188770e+01             -0.090088\n",
            "12   ETH - MATIC        -2.426916e+03             -0.837723\n",
            "13   DOGE - ADA          1.196409e+11              1.337782\n",
            "14   DOGE - LINK         8.417969e-02              1.525924\n",
            "15    DOGE - TRX        -3.603906e+00              1.285568\n",
            "16    DOGE - UNI         1.904297e-02              1.177559\n",
            "17  DOGE - MATIC         3.047070e+00             -0.232798\n",
            "18   ADA  - LINK         6.597070e+11              1.541582\n",
            "19    ADA  - TRX        -3.458584e+01              0.827469\n",
            "20    ADA  - UNI         5.634766e-02              0.999836\n",
            "21  ADA  - MATIC        -3.542496e+12              0.610116\n",
            "22    LINK - TRX        -5.035851e+03              1.040077\n",
            "23    LINK - UNI         1.268652e+00              1.079396\n",
            "24  LINK - MATIC        -5.497558e+12              0.610116\n",
            "25     TRX - UNI         1.806641e-03              0.777654\n",
            "26   TRX - MATIC         2.171535e+12              0.610116\n",
            "27   UNI - MATIC         2.325410e+01             -0.069090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6KVmh9B0iR--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5_NQNtpPiR8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xpf5PfcciR5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ILggxIgSiR2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TXhJVvIqiRzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lcc4easUiRwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mb_zcl7MiRtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate spreads and z-scores\n",
        "def calculate_spread_and_zscore(pairs, data):\n",
        "    spread_results = []\n",
        "    zscore_results = []\n",
        "    optimal_entries = []\n",
        "    optimal_exits = []\n",
        "\n",
        "    for pair in pairs:\n",
        "        asset1_name, asset2_name, pvalue = pair\n",
        "        if asset1_name not in data.columns or asset2_name not in data.columns:\n",
        "            raise ValueError(f\"Asset names {asset1_name} or {asset2_name} not found in data columns.\")\n",
        "\n",
        "        asset1 = data[asset1_name]\n",
        "        asset2 = data[asset2_name]\n",
        "\n",
        "        asset1, asset2 = asset1.align(asset2, join='inner')\n",
        "        combined = pd.DataFrame({'asset1': asset1.ffill(), 'asset2': asset2.ffill()}).dropna()\n",
        "\n",
        "        if combined.empty:\n",
        "            continue\n",
        "\n",
        "        asset1_cleaned = combined['asset1']\n",
        "        asset2_cleaned = combined['asset2']\n",
        "\n",
        "        spread = asset1_cleaned - asset2_cleaned\n",
        "        spread_mean = spread.mean()\n",
        "        spread_std = spread.std()\n",
        "\n",
        "        optimal_entry = (spread_std * 2.0)\n",
        "        optimal_exit = (spread_std * 0.5)\n",
        "\n",
        "        if spread_std == 0:\n",
        "            zscore = pd.Series([0] * len(spread), index=spread.index)\n",
        "        else:\n",
        "            zscore = (spread - spread_mean) / spread_std\n",
        "\n",
        "        spread_results.append(spread)\n",
        "        zscore_results.append(zscore)\n",
        "        optimal_entries.append(optimal_entry)\n",
        "        optimal_exits.append(optimal_exit)\n",
        "\n",
        "    return spread_results, zscore_results, optimal_entries, optimal_exits"
      ],
      "metadata": {
        "id": "kDpxmw7dHUQ1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iQZbeMVsHUOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xPCruekUHULQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NOMpjN6WHUIN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}